{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":39911,"sourceType":"datasetVersion","datasetId":31296},{"sourceId":8684034,"sourceType":"datasetVersion","datasetId":4942251}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"#### 0. Import library, and define constants","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport string\nimport re\nimport seaborn as sns\nimport torchvision\nimport os\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nimport cv2\nimport matplotlib.pyplot as plt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"INPUT_IMAGES_DIR = \"/kaggle/input/flickr-image-dataset/flickr30k_images/flickr30k_images/\"\nLABEL_PATH = \"/kaggle/input/flickr-image-dataset/flickr30k_images/results.csv\"\nOUTPUT_PATH = \"/kaggle/working\"\n\nUNK = \"#UNK\"\nPAD = \"#PAD\"\nSTART = \"#START\"\nEND = \"#END\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(LABEL_PATH, sep=\"|\")\ndf.dropna(inplace=True)\ndf.reset_index(drop=True, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!pip3 install googletrans==3.1.0a0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#from googletrans import Translator, constants\n\n#translator = Translator()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#half_of_text = df[' comment'].iloc[0:int(df.shape[0]/2)].apply(lambda x: translator.translate(x, dest=\"ru\").text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#pd.DataFrame(half_of_text).to_csv(r'/kaggle/working/half_data.csv', index= False )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#second_half = df[' comment'].iloc[int(df.shape[0]/2):].apply(lambda x: translator.translate(x, dest=\"ru\").text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#pd.DataFrame(second_half).to_csv(r'/kaggle/working/half_data2.csv', index= False )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_half1 = pd.read_csv('/kaggle/input/half-data-translated/half_data.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_half1.columns = [' comment']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_half2 = pd.read_csv('/kaggle/input/half-data-translated/half_data2.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_half2['index'] = list(range(79457, 158914))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_half2 = df_half2.set_index('index')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[' comment'] = pd.DataFrame(pd.concat([df_half1, df_half2]).reset_index(drop=True))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 1. Data visualization & analysis","metadata":{}},{"cell_type":"code","source":"regex = re.compile('[%s]' % re.escape(string.punctuation))\ndef clean_text(row):\n    row = str(row).strip()\n    row = row.lower()\n    return regex.sub(\"\", row)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.columns = [col.strip() for col in df.columns]\ndf[\"comment\"] = df[\"comment\"].apply(clean_text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"length\"] = df[\"comment\"].apply(lambda row: len(row.strip().split()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.displot(data=df, x='length', palette='mako', kind='kde', fill=True, aspect=4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"captions = df[\"comment\"].tolist()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"captions[:10]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word_freq = {}\nfor caption in captions:\n    caption = caption.strip()\n    for word in caption.split():\n        if word not in word_freq:\n            word_freq[word] = 0\n        word_freq[word] += 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dict(sorted(word_freq.items(), key=lambda item: item[1])[:30])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dict(sorted(word_freq.items(), key=lambda item: item[1], reverse=True)[:30])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 2. Data preparation","metadata":{}},{"cell_type":"code","source":"def build_vocab(captions, word_freq, count_threshold=5):\n    vocab = {\n        PAD: 0,\n        UNK: 1,\n        START: 2,\n        END: 3\n    }\n    index = 4\n    \n    for caption in captions:\n        caption = caption.strip().split(\" \")\n        for word in caption:\n            if word and word_freq[word] >= count_threshold and word not in vocab:\n                vocab[word] = index\n                index += 1\n\n    inv_vocab = {v: k for k, v in vocab.items()}\n    return vocab, inv_vocab","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocab, inv_vocab = build_vocab(captions, word_freq)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def convert_captions(captions, vocab, max_length=30):\n    tokens = [[vocab[PAD]]*max_length for _ in range(len(captions))]\n    for i, caption in enumerate(captions):\n        caption = caption.strip().split()\n        tokens[i][0] = vocab[START]\n        j = 1\n        for word in caption[:max_length-2]:\n            if word not in vocab:\n                tokens[i][j] = vocab[UNK]\n            else:\n                tokens[i][j] = vocab[word]\n            j += 1\n        tokens[i][j] = vocab[END]\n    return tokens","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokens = convert_captions(captions, vocab)\nimg_paths = list(df[\"image_name\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ImageCaptioningDataset(torch.utils.data.Dataset):\n    \n    def __init__(self, img_paths, tokens):\n        \"\"\"\n        img_paths: a list of image path we get from dataframe\n        tokens: a list of tokens that we converted from text captions\n        \"\"\"\n        self.img_paths = [os.path.join(INPUT_IMAGES_DIR, p) for p in img_paths]\n        self.tokens = tokens\n        assert len(self.img_paths) == len(self.tokens), \"Make sure len(img_paths) == len(tokens).\"\n    \n    def __getitem__(self, index):\n        \"\"\"\n        Get image path and token. Then load image path to numpy array image. Convert to pytorch tensor if it's necessary. \n        \"\"\"\n        img_path = self.img_paths[index]\n        token = self.tokens[index]\n        img = cv2.imread(img_path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        img = self._resize_img(img, shape=(300, 300))\n        img = torchvision.transforms.ToTensor()(img)\n        token = torch.as_tensor(token)\n        return img, token\n    \n    def __len__(self):\n        return len(self.img_paths)\n\n    def _resize_img(self, img, shape=(300, 300)):\n        h, w = img.shape[0], img.shape[1]\n        pad_left = 0\n        pad_right = 0\n        pad_top = 0\n        pad_bottom = 0\n        if h > w:\n            diff = h - w\n            pad_top = diff - diff // 2\n            pad_bottom = diff // 2\n        else:\n            diff = w - h\n            pad_left = diff - diff // 2\n            pad_right = diff // 2\n        cropped_img = img[pad_top:h-pad_bottom, pad_left:w-pad_right, :]\n        cropped_img = cv2.resize(cropped_img, shape)\n        return cropped_img","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = ImageCaptioningDataset(img_paths, tokens)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 3. Model architecture","metadata":{}},{"cell_type":"markdown","source":"![Image Captioning Model](https://raw.githubusercontent.com/yunjey/pytorch-tutorial/master/tutorials/03-advanced/image_captioning/png/model.png)","metadata":{}},{"cell_type":"code","source":"MAX_LENGTH = 30\nNUM_VOCAB = len(vocab)\nBATCH_SIZE = 128\nEPOCH = 5\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### 3.1 Define CNN encoder class:","metadata":{}},{"cell_type":"code","source":"from torchvision import models\n\nclass CNNEncoder(nn.Module):\n\n    def __init__(self) -> None:\n        super().__init__()\n        self.cnn = models.resnet34(pretrained=True)\n\n    def forward(self, img):\n        return self.cnn(img)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### 3.2 Define LSTM decoder class:","metadata":{}},{"cell_type":"code","source":"class RNNDecoder(nn.Module):\n\n    def __init__(self, num_vocab) -> None:\n        super().__init__()\n        self.bottleneck = nn.Sequential(\n            nn.Linear(1000, 512),\n            nn.ReLU(),\n            nn.Linear(512, 256),\n            nn.ReLU()\n        )\n        self.num_vocab = num_vocab\n        self.embedding = nn.Embedding(num_embeddings=num_vocab, embedding_dim=256, padding_idx=0)\n        self.num_layers = 1\n        self.bidirectional = False\n        self.rnn = nn.LSTM(input_size=256, hidden_size=256, num_layers=self.num_layers, batch_first=True, bidirectional=self.bidirectional)\n        self.classifier = nn.Sequential(\n            nn.Linear(256, 512),\n            nn.ReLU(),\n            nn.Linear(512, 1024),\n            nn.ReLU(),\n            nn.Linear(1024, 2048),\n            nn.ReLU(),\n            nn.Linear(2048, num_vocab)\n        )\n        self.softmax = nn.Softmax(dim=-1)\n\n    def forward(self, input, img_embeded, prediction=False):\n        img_embeded = self.bottleneck(img_embeded)\n        img_embeded = torch.stack([img_embeded]*(self.num_layers), dim=0)\n        if prediction:\n            output = []\n            hidden = (img_embeded, img_embeded)\n            out = input\n            while out != vocab[END] and len(output) <= MAX_LENGTH:\n                out = torch.tensor([[out]]).to(\"cuda\")\n                out = self.embedding(out)\n                out, hidden = self.rnn(out, hidden)\n                out = self.classifier(out)\n                out = self.softmax(out)\n                out = torch.argmax(out, dim=-1)\n                out = out.squeeze().item()\n                output.append(out)\n        else:\n            input = self.embedding(input)\n            output, (h, c) = self.rnn(input, (img_embeded, img_embeded))\n            output = self.classifier(output)\n        return output","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 4. Train model","metadata":{}},{"cell_type":"code","source":"class ImageCaptioningModel:\n\n    def __init__(self, encoder : CNNEncoder, decoder : RNNDecoder, train_dataset : ImageCaptioningDataset):\n        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        self.encoder = encoder.to(self.device)\n        self.encoder.eval()\n        self.decoder = decoder.to(self.device)\n        self.train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n        self.optimizer = optim.Adam(decoder.parameters(), lr=0.000001)\n        self.loss = nn.CrossEntropyLoss()\n\n    def clean_prediction(self, prediction):\n        special_tokens = [\"#UNK\", \"#PAD\", \"#START\", \"#END\"]\n        for token in special_tokens:\n            prediction = prediction.replace(token, \"\")\n        return prediction.strip()\n\n    def predict(self, img):\n        with torch.no_grad():\n            img_embed = self.encoder(img)\n            caption = vocab[START]\n            caption = self.decoder(caption, img_embed, prediction=True)\n        \n        text = [inv_vocab[t] for t in caption]\n        text = \" \".join(text)\n        return self.clean_prediction(text)\n    \n    def save_model(self, encoder_path: str, decoder_path: str, optimizer_path: str):\n        torch.save(self.encoder.state_dict(), encoder_path)\n        torch.save(self.decoder.state_dict(), decoder_path)\n        torch.save(self.optimizer.state_dict(), optimizer_path)\n    \n    def load_model(self, encoder_path: str, decoder_path: str, optimizer_path: str):\n        self.encoder.load_state_dict(torch.load(encoder_path))\n        self.decoder.load_state_dict(torch.load(decoder_path))\n        self.optimizer.load_state_dict(torch.load(optimizer_path))\n        self.encoder.to(self.device)\n        self.decoder.to(self.device)\n    \n    def train(self):\n        for e in range(EPOCH):\n            pbar = tqdm(self.train_dataloader, desc=\"Epoch: {}\".format(e+1))\n            for i, (img, caption) in enumerate(pbar):\n                img = img.to(self.device)\n                caption = caption.to(self.device)\n                img_embed = self.encoder(img)\n                output = self.decoder(caption[:, :-1], img_embed)\n                output = output.permute(0, 2, 1)\n                loss = self.loss(output, caption[:, 1:])\n\n                self.optimizer.zero_grad()\n                loss.backward() \n                self.optimizer.step()\n\n                pbar.set_description(desc=\"Epoch \" + str(e+1) + \" - Loss: %.5f\" % (loss.item()))\n                \n                if ((i+1)%100) == 0:\n                    plt.imshow(img[-1].cpu().detach().numpy().transpose((1, 2, 0)))\n                    output = self.predict(img[-1].unsqueeze(0))\n                    plt.title(output)\n                    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cnn = CNNEncoder()\nrnn = RNNDecoder(num_vocab=NUM_VOCAB)\nmodel = ImageCaptioningModel(encoder=cnn, decoder=rnn, train_dataset=dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#model = torch.load('/kaggle/input/half-data-translated/model_rgb.pth')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#model.train()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#model.save_model('encoder_weights.pth', 'decoder_weights.pth', 'optimizer_state.pth')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#torch.save(model, 'model_rgb.pth')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\n%cd /kaggle/working\nfrom IPython.display import FileLink\nFileLink('model_rgb.pth')\n'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#model.load_model('/kaggle/input/half-data-translated/encoder_weights.pth', '/kaggle/input/half-data-translated/decoder_weights.pth', '/kaggle/input/half-data-translated/optimizer_state.pth')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#model = torch.load('/kaggle/input/half-data-translated/model_rgb.pth')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 5. Predict","metadata":{}},{"cell_type":"code","source":"#img = dataset.__getitem__(10)[0].unsqueeze(0)\n#caption = model.predict(img.to(DEVICE))\n#print(caption)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install streamlit","metadata":{"execution":{"iopub.status.busy":"2024-06-13T21:49:32.898739Z","iopub.execute_input":"2024-06-13T21:49:32.899101Z","iopub.status.idle":"2024-06-13T21:49:48.042273Z","shell.execute_reply.started":"2024-06-13T21:49:32.899068Z","shell.execute_reply":"2024-06-13T21:49:48.041335Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting streamlit\n  Downloading streamlit-1.35.0-py2.py3-none-any.whl.metadata (8.5 kB)\nRequirement already satisfied: altair<6,>=4.0 in /opt/conda/lib/python3.10/site-packages (from streamlit) (5.3.0)\nRequirement already satisfied: blinker<2,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from streamlit) (1.8.2)\nRequirement already satisfied: cachetools<6,>=4.0 in /opt/conda/lib/python3.10/site-packages (from streamlit) (4.2.4)\nRequirement already satisfied: click<9,>=7.0 in /opt/conda/lib/python3.10/site-packages (from streamlit) (8.1.7)\nRequirement already satisfied: numpy<2,>=1.19.3 in /opt/conda/lib/python3.10/site-packages (from streamlit) (1.26.4)\nRequirement already satisfied: packaging<25,>=16.8 in /opt/conda/lib/python3.10/site-packages (from streamlit) (21.3)\nRequirement already satisfied: pandas<3,>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from streamlit) (2.2.1)\nRequirement already satisfied: pillow<11,>=7.1.0 in /opt/conda/lib/python3.10/site-packages (from streamlit) (9.5.0)\nRequirement already satisfied: protobuf<5,>=3.20 in /opt/conda/lib/python3.10/site-packages (from streamlit) (3.20.3)\nRequirement already satisfied: pyarrow>=7.0 in /opt/conda/lib/python3.10/site-packages (from streamlit) (14.0.2)\nRequirement already satisfied: requests<3,>=2.27 in /opt/conda/lib/python3.10/site-packages (from streamlit) (2.32.3)\nRequirement already satisfied: rich<14,>=10.14.0 in /opt/conda/lib/python3.10/site-packages (from streamlit) (13.7.0)\nRequirement already satisfied: tenacity<9,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from streamlit) (8.2.3)\nRequirement already satisfied: toml<2,>=0.10.1 in /opt/conda/lib/python3.10/site-packages (from streamlit) (0.10.2)\nRequirement already satisfied: typing-extensions<5,>=4.3.0 in /opt/conda/lib/python3.10/site-packages (from streamlit) (4.9.0)\nRequirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /opt/conda/lib/python3.10/site-packages (from streamlit) (3.1.41)\nCollecting pydeck<1,>=0.8.0b4 (from streamlit)\n  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\nRequirement already satisfied: tornado<7,>=6.0.3 in /opt/conda/lib/python3.10/site-packages (from streamlit) (6.3.3)\nCollecting watchdog>=2.1.5 (from streamlit)\n  Downloading watchdog-4.0.1-py3-none-manylinux2014_x86_64.whl.metadata (37 kB)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from altair<6,>=4.0->streamlit) (3.1.2)\nRequirement already satisfied: jsonschema>=3.0 in /opt/conda/lib/python3.10/site-packages (from altair<6,>=4.0->streamlit) (4.20.0)\nRequirement already satisfied: toolz in /opt/conda/lib/python3.10/site-packages (from altair<6,>=4.0->streamlit) (0.12.1)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.11)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging<25,>=16.8->streamlit) (3.1.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas<3,>=1.3.0->streamlit) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas<3,>=1.3.0->streamlit) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas<3,>=1.3.0->streamlit) (2023.4)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.27->streamlit) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.27->streamlit) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.27->streamlit) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.27->streamlit) (2024.2.2)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich<14,>=10.14.0->streamlit) (2.17.2)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->altair<6,>=4.0->streamlit) (2.1.3)\nRequirement already satisfied: attrs>=22.2.0 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (23.2.0)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2023.12.1)\nRequirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.32.1)\nRequirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.16.2)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas<3,>=1.3.0->streamlit) (1.16.0)\nDownloading streamlit-1.35.0-py2.py3-none-any.whl (8.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m76.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m91.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading watchdog-4.0.1-py3-none-manylinux2014_x86_64.whl (83 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.0/83.0 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: watchdog, pydeck, streamlit\nSuccessfully installed pydeck-0.9.1 streamlit-1.35.0 watchdog-4.0.1\n","output_type":"stream"}]},{"cell_type":"code","source":"!npm install localtunnel","metadata":{"execution":{"iopub.status.busy":"2024-06-13T21:49:48.043975Z","iopub.execute_input":"2024-06-13T21:49:48.044280Z","iopub.status.idle":"2024-06-13T21:49:51.270597Z","shell.execute_reply.started":"2024-06-13T21:49:48.044254Z","shell.execute_reply":"2024-06-13T21:49:51.269687Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"\u001b[K\u001b[?25hm##################\u001b[0m] \\ reify:axios: \u001b[32;40mhttp\u001b[0m \u001b[35mfetch\u001b[0m GET 200 https://registry.npmjs.o\u001b[0m\u001b[Kistry.\u001b[0m\u001b[K\nadded 22 packages in 1s\n\n3 packages are looking for funding\n  run `npm fund` for details\n\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[36;40mnotice\u001b[0m\u001b[35m\u001b[0m \n\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[36;40mnotice\u001b[0m\u001b[35m\u001b[0m New \u001b[33mminor\u001b[39m version of npm available! \u001b[31m10.1.0\u001b[39m -> \u001b[32m10.8.1\u001b[39m\n\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[36;40mnotice\u001b[0m\u001b[35m\u001b[0m Changelog: \u001b[36mhttps://github.com/npm/cli/releases/tag/v10.8.1\u001b[39m\n\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[36;40mnotice\u001b[0m\u001b[35m\u001b[0m Run \u001b[32mnpm install -g npm@10.8.1\u001b[39m to update!\n\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[36;40mnotice\u001b[0m\u001b[35m\u001b[0m \n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"%%writefile app.py\n\nimport streamlit as st\nfrom PIL import Image\nfrom torchvision import models\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport string\nimport re\nimport torchvision\nimport os\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nimport cv2\nimport json\nfrom torchvision import models\n\nINPUT_IMAGES_DIR = \"/kaggle/input/flickr-image-dataset/flickr30k_images/flickr30k_images/\"\nLABEL_PATH = \"/kaggle/input/flickr-image-dataset/flickr30k_images/results.csv\"\nOUTPUT_PATH = \"/kaggle/working\"\n\nUNK = \"#UNK\"\nPAD = \"#PAD\"\nSTART = \"#START\"\nEND = \"#END\"\n\n#f = open(\"/kaggle/working/vocab.json\", \"r\")\n#vocab = json.load(f)\n#f = open(\"/kaggle/working/inv_vocab.json\", \"r\")\n#inv_vocab = json.load(f)\n\ndf = pd.read_csv(LABEL_PATH, sep=\"|\")\ndf.dropna(inplace=True)\ndf.reset_index(drop=True, inplace=True)\n\ndf_half1 = pd.read_csv('/kaggle/input/half-data-translated/half_data.csv')\ndf_half1.columns = [' comment']\ndf_half2 = pd.read_csv('/kaggle/input/half-data-translated/half_data2.csv')\ndf_half2['index'] = list(range(79457, 158914))\ndf_half2 = df_half2.set_index('index')\ndf[' comment'] = pd.DataFrame(pd.concat([df_half1, df_half2]).reset_index(drop=True))\nregex = re.compile('[%s]' % re.escape(string.punctuation))\ndef clean_text(row):\n    row = str(row).strip()\n    row = row.lower()\n    return regex.sub(\"\", row)\ndf.columns = [col.strip() for col in df.columns]\ndf[\"comment\"] = df[\"comment\"].apply(clean_text)\ncaptions = df[\"comment\"].tolist()\n\nword_freq = {}\nfor caption in captions:\n    caption = caption.strip()\n    for word in caption.split():\n        if word not in word_freq:\n            word_freq[word] = 0\n        word_freq[word] += 1\n        \ndef build_vocab(captions, word_freq, count_threshold=5):\n    vocab = {\n        PAD: 0,\n        UNK: 1,\n        START: 2,\n        END: 3\n    }\n    index = 4\n    \n    for caption in captions:\n        caption = caption.strip().split(\" \")\n        for word in caption:\n            if word and word_freq[word] >= count_threshold and word not in vocab:\n                vocab[word] = index\n                index += 1\n\n    inv_vocab = {v: k for k, v in vocab.items()}\n    return vocab, inv_vocab\n\nvocab, inv_vocab = build_vocab(captions, word_freq)\n\ndef convert_captions(captions, vocab, max_length=30):\n    tokens = [[vocab[PAD]]*max_length for _ in range(len(captions))]\n    for i, caption in enumerate(captions):\n        caption = caption.strip().split()\n        tokens[i][0] = vocab[START]\n        j = 1\n        for word in caption[:max_length-2]:\n            if word not in vocab:\n                tokens[i][j] = vocab[UNK]\n            else:\n                tokens[i][j] = vocab[word]\n            j += 1\n        tokens[i][j] = vocab[END]\n    return tokens\n\ntokens = convert_captions(captions, vocab)\nimg_paths = list(df[\"image_name\"])\n\nMAX_LENGTH = 30\nNUM_VOCAB = len(vocab)\nBATCH_SIZE = 128\nEPOCH = 5\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n\n\nclass ImageCaptioningDataset(torch.utils.data.Dataset):\n    \n    def __init__(self, img_paths, tokens):\n        \"\"\"\n        img_paths: a list of image path we get from dataframe\n        tokens: a list of tokens that we converted from text captions\n        \"\"\"\n        self.img_paths = [os.path.join(INPUT_IMAGES_DIR, p) for p in img_paths]\n        self.tokens = tokens\n        assert len(self.img_paths) == len(self.tokens), \"Make sure len(img_paths) == len(tokens).\"\n    \n    def __getitem__(self, index):\n        \"\"\"\n        Get image path and token. Then load image path to numpy array image. Convert to pytorch tensor if it's necessary. \n        \"\"\"\n        img_path = self.img_paths[index]\n        token = self.tokens[index]\n        img = cv2.imread(img_path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        img = self._resize_img(img, shape=(300, 300))\n        img = torchvision.transforms.ToTensor()(img)\n        token = torch.as_tensor(token)\n        return img, token\n    \n    def __len__(self):\n        return len(self.img_paths)\n\n    def _resize_img(self, img, shape=(300, 300)):\n        h, w = img.shape[0], img.shape[1]\n        pad_left = 0\n        pad_right = 0\n        pad_top = 0\n        pad_bottom = 0\n        if h > w:\n            diff = h - w\n            pad_top = diff - diff // 2\n            pad_bottom = diff // 2\n        else:\n            diff = w - h\n            pad_left = diff - diff // 2\n            pad_right = diff // 2\n        cropped_img = img[pad_top:h-pad_bottom, pad_left:w-pad_right, :]\n        cropped_img = cv2.resize(cropped_img, shape)\n        return cropped_img\n\ndataset = ImageCaptioningDataset(img_paths, tokens)\n\nclass CNNEncoder(nn.Module):\n\n    def __init__(self) -> None:\n        super().__init__()\n        self.cnn = models.resnet34(pretrained=True)\n\n    def forward(self, img):\n        return self.cnn(img)\nclass RNNDecoder(nn.Module):\n\n    def __init__(self, num_vocab) -> None:\n        super().__init__()\n        self.bottleneck = nn.Sequential(\n            nn.Linear(1000, 512),\n            nn.ReLU(),\n            nn.Linear(512, 256),\n            nn.ReLU()\n        )\n        self.num_vocab = num_vocab\n        self.embedding = nn.Embedding(num_embeddings=num_vocab, embedding_dim=256, padding_idx=0)\n        self.num_layers = 1\n        self.bidirectional = False\n        self.rnn = nn.LSTM(input_size=256, hidden_size=256, num_layers=self.num_layers, batch_first=True, bidirectional=self.bidirectional)\n        self.classifier = nn.Sequential(\n            nn.Linear(256, 512),\n            nn.ReLU(),\n            nn.Linear(512, 1024),\n            nn.ReLU(),\n            nn.Linear(1024, 2048),\n            nn.ReLU(),\n            nn.Linear(2048, num_vocab)\n        )\n        self.softmax = nn.Softmax(dim=-1)\n\n    def forward(self, input, img_embeded, prediction=False):\n        img_embeded = self.bottleneck(img_embeded)\n        img_embeded = torch.stack([img_embeded]*(self.num_layers), dim=0)\n        if prediction:\n            output = []\n            hidden = (img_embeded, img_embeded)\n            out = input\n            while out != vocab[END] and len(output) <= MAX_LENGTH:\n                out = torch.tensor([[out]]).to(\"cuda\")\n                out = self.embedding(out)\n                out, hidden = self.rnn(out, hidden)\n                out = self.classifier(out)\n                out = self.softmax(out)\n                out = torch.argmax(out, dim=-1)\n                out = out.squeeze().item()\n                output.append(out)\n        else:\n            input = self.embedding(input)\n            output, (h, c) = self.rnn(input, (img_embeded, img_embeded))\n            output = self.classifier(output)\n        return output\n\nclass ImageCaptioningModel:\n\n    def __init__(self, encoder : CNNEncoder, decoder : RNNDecoder, train_dataset : ImageCaptioningDataset):\n        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        self.encoder = encoder.to(self.device)\n        self.encoder.eval()\n        self.decoder = decoder.to(self.device)\n        self.train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n        self.optimizer = optim.Adam(decoder.parameters())\n        self.loss = nn.CrossEntropyLoss()\n\n    def clean_prediction(self, prediction):\n        special_tokens = [\"#UNK\", \"#PAD\", \"#START\", \"#END\"]\n        for token in special_tokens:\n            prediction = prediction.replace(token, \"\")\n        return prediction.strip()\n\n    def predict(self, img):\n        with torch.no_grad():\n            img_embed = self.encoder(img)\n            caption = vocab[START]\n            caption = self.decoder(caption, img_embed, prediction=True)\n        \n        text = [inv_vocab[t] for t in caption]\n        text = \" \".join(text)\n        return self.clean_prediction(text)\n    \n    def save_model(self, encoder_path: str, decoder_path: str, optimizer_path: str):\n        torch.save(self.encoder.state_dict(), encoder_path)\n        torch.save(self.decoder.state_dict(), decoder_path)\n        torch.save(self.optimizer.state_dict(), optimizer_path)\n    \n    def load_model(self, encoder_path: str, decoder_path: str, optimizer_path: str):\n        self.encoder.load_state_dict(torch.load(encoder_path))\n        self.decoder.load_state_dict(torch.load(decoder_path))\n        self.optimizer.load_state_dict(torch.load(optimizer_path))\n        self.encoder.to(self.device)\n        self.decoder.to(self.device)\n    \n    def train(self):\n        for e in range(EPOCH):\n            pbar = tqdm(self.train_dataloader, desc=\"Epoch: {}\".format(e+1))\n            for i, (img, caption) in enumerate(pbar):\n                img = img.to(self.device)\n                caption = caption.to(self.device)\n                img_embed = self.encoder(img)\n                output = self.decoder(caption[:, :-1], img_embed)\n                output = output.permute(0, 2, 1)\n                loss = self.loss(output, caption[:, 1:])\n\n                self.optimizer.zero_grad()\n                loss.backward() \n                self.optimizer.step()\n\n                pbar.set_description(desc=\"Epoch \" + str(e+1) + \" - Loss: %.5f\" % (loss.item()))\n                \n                if ((i+1)%100) == 0:\n                    plt.imshow(img[-1].cpu().detach().numpy().transpose((1, 2, 0)))\n                    output = self.predict(img[-1].unsqueeze(0))\n                    plt.title(output)\n                    plt.show()\n\nmodel = torch.load('/kaggle/input/half-data-translated/model_rgb.pth')\n\ndef resize_img(img, shape=(300, 300)):\n    h, w = img.shape[0], img.shape[1]\n    pad_left = 0\n    pad_right = 0\n    pad_top = 0\n    pad_bottom = 0\n    if h > w:\n        diff = h - w\n        pad_top = diff - diff // 2\n        pad_bottom = diff // 2\n    else:\n        diff = w - h\n        pad_left = diff - diff // 2\n        pad_right = diff // 2\n    cropped_img = img[pad_top:h-pad_bottom, pad_left:w-pad_right, :]\n    cropped_img = cv2.resize(cropped_img, shape)\n    return cropped_img\n\nst.title(\"Image Caption Generator\")\nst.write(\"Upload an image to generate a caption\")\n\n# Загрузка изображения\nuploaded_file = st.file_uploader(\"Choose an image...\", type=\"jpg\")\n\nif uploaded_file is not None:\n    image = Image.open(uploaded_file)\n    st.image(image, caption='Uploaded Image.', use_column_width=True)\n    st.write(\"\")\n    st.write(\"Generating caption...\")\n    image = np.array(image)\n    image = resize_img(image, shape=(image.shape[0], image.shape[1]))\n    image = torchvision.transforms.ToTensor()(image)\n    image = image.unsqueeze(0)\n    caption = model.predict(image.to(DEVICE))\n    st.write(caption)","metadata":{"execution":{"iopub.status.busy":"2024-06-13T21:49:51.272655Z","iopub.execute_input":"2024-06-13T21:49:51.273049Z","iopub.status.idle":"2024-06-13T21:49:51.287355Z","shell.execute_reply.started":"2024-06-13T21:49:51.273011Z","shell.execute_reply":"2024-06-13T21:49:51.286206Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Writing app.py\n","output_type":"stream"}]},{"cell_type":"code","source":"!curl ipv4.icanhazip.com","metadata":{"execution":{"iopub.status.busy":"2024-06-13T21:49:51.289445Z","iopub.execute_input":"2024-06-13T21:49:51.289757Z","iopub.status.idle":"2024-06-13T21:49:52.278662Z","shell.execute_reply.started":"2024-06-13T21:49:51.289728Z","shell.execute_reply":"2024-06-13T21:49:52.277628Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"34.83.229.11\n","output_type":"stream"}]},{"cell_type":"code","source":"#!streamlit run app.py &>./logs.txt & npx localtunnel --port 8501","metadata":{"execution":{"iopub.status.busy":"2024-06-13T21:50:07.927597Z","iopub.execute_input":"2024-06-13T21:50:07.928356Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"your url is: https://loud-papayas-lose.loca.lt\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}